% Copyright (c) 2023 Ignacio Slater Mu√±oz All rights reserved.
% Use of this source code is governed by a BSD-style
% license that can be found in the LICENSE file.

\subsubsection{Evaluation}
\label{sec:bg:gp:repr_ev:eval}
  To determine a program's fitness, various methods exist, but a prevalent 
  choice is the \emph{mean squared error} (MSE) between the points and the 
  program.

  \begin{definition}[Mean Squared Error]
  \label{def:mse}
    Given a vector of \(n\) predictions derived from \(n\) data points across 
    all variables, where \(\mathbf{y}_i\) represents the \(i\)-th observed 
    value and \(\hat{\mathbf{y}}_i\) is the \(i\)-th prediction, the MSE of the 
    predictor is a function \(\mathrm{MSE}:\: \mathbb{R}^n \times \mathbb{R}^n 
    \to \mathbb{R}\) defined by:

    \begin{equation}
    \label{eq:mse}
      \mathrm{MSE}(\mathbf{y}, \hat{\mathbf{y}}) = 
        \frac{1}{n} \sum_{i=1}^{n} (\mathbf{y}_i - \hat{\mathbf{y}}_i)^2
    \end{equation}
  \end{definition}

  The MSE is a benchmark for assessing an estimator's quality, especially in 
  \textit{machine learning} contexts.\footnote{
    While we favor the mean squared error for its straightforward nature and 
    prominence in literature, other error measures like the \textit{mean 
    absolute error} (MAE) or the \textit{cross-entropy} (CE) loss functions can 
    also be considered.
  }

  For our purposes, we employ the MSE to assess a program's fitness. Given a 
  program \(\mathsf{P}\) and two point sets, \(\mathbf{x}\) and \(\mathbf{y}\), 
  as detailed in \vref{tab:bg:gp:repr_ev:points}, and presuming 
  \(\mathsf{P}[\mathbf{x}]\) as the points produced by evaluating 
  \(\mathsf{P}\) on \(\mathbf{x}\), with \(\mathsf{P}(x)\) being the evaluation 
  result for point \(x\), the fitness of \(\mathsf{P}\) can be described as:

  \begin{equation}
  \label{eq:bg:gp:repr_ev:fitness}
    \phi_\mathsf{P} = \mathrm{MSE}(\mathbf{y}, \mathsf{P}[\mathbf{x}]) = 
      \frac{1}{n} \sum_{i=1}^{n} (\mathbf{y}_i - \mathsf{P}(\mathbf{x}_i))^2
  \end{equation}

  In summation, when gauging a program's fitness in GP, error measures play a 
  pivotal role, with MSE emerging as a primary standard. By contrasting a 
  program's predictions with actual data, the MSE offers a valuable assessment 
  of prediction accuracy. Its inherent simplicity effectively establishes a 
  quality standard for future enhancements. Such an organized methodology 
  guarantees that GP systematically traverses the solution domain, persistently 
  optimizing candidate solutions.
